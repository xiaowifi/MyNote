> 视频是封装格式。

* 视频属于压缩格式(编码)H264 视频画面
* aac 音频文件 
* yuv 色彩和RGBA区别，在视频相关领域，视频采用YUV 色彩编码，主要是RGBA 表示一个点的颜色成本较大。
* I 帧 （最大）存储编码数据和预测数据。视频的第一帧和关键帧,I 帧越多，相对视频文件越大，当然视频文件大小还和帧频有关。直播的I帧比较频繁，但是帧频较低、
* P 帧 运动矢量+ 差异数据（类似于编码数据）
* B 帧 运动矢量 最小。B帧越多，运算越多，越耗时。
* 每一帧都是由多个宏块组成  
* 通常帧是 IPB  这种顺序，但是不是播放顺序。
* pts 在ffmpeg 上记录视频的视(音)频播放帧 单位纳秒
* sps 配置信息，通常是I帧才有PSP和pps 
* pps 宽高 做直播的时候，sps 可能拿不到，所以pps 一定有。
* H265 压缩比例比H264 大，因为预测数据可支持方位更多，宏块区域更大。
* 当前环境下H264编码视频瓶颈
    * 视频分辨率变大，720到4K,8K，宏块最多是16*16，运动矢量预测是单方向预测。宏块数量变多，但是复杂度降低。但是运动矢量复杂度变高，P帧和B帧内容变多。视频依旧较大。宏块变多，解码速度降低。
    * 视频帧频变高，30到60到 120等等。
    * 宏块压缩算法还是以单个宏块进行预测压缩。
* H265 特点
    * 宏块不再局限于最多 16*16，宏块最多可以达到64*64,最低依旧是4*4.预测方向变多(对一个宏块细节越复杂，通过树状结构绑定的宏块数量越多)。视频文件变小。
    * 解析和压缩成本变高。
    * 宏块预测方向变多后，相对与H624清晰度会变高。细节表达上会更清楚。同时保留最多9个方向预测。
    * I 帧相对于H264更大。P帧和B帧相对于H264小很多。
    * 可以做3D 效果，I帧前可以存储 VPS 信息 
    * H265设备收费？
* H246 文件属于已经编码(压缩后)好的数据。
* 播放过程就是解码的过程
* MediaCodec 是Android提供的对于音视频进行编码解码的类，它通过访问底层的编解码器实现音视频的功能。 是Android Media集成框架的一部分。
* 底层==专门的设备  视频的编码解码是统一的。所以通过物理硬件进行编码解码。
* 移动端Soc的视频硬解码是靠Soc里面的Dsp芯片。不是GPC也不是CPU
* 硬解码是指系统将H264或者aac音频文件交给DSP芯片进行处理，处理好之后渲染到屏幕上就是视频硬解码过程。
* MediaCodec 支持编码与解码，catch 的问题之一就是 当前设备dps芯片不支持当前解码，方式是通过软解。
* 编码格式video/avc 在 mediaFormat 中可以找到对应的编码方式。
* mediaFormat 视频配置信息。
* mediaExtractor 可以获取到完整的视频的宽高，H264文件无法通过这种方式获取宽高。
* 获取到转码的内容是ByteBuffer,不可用直接使用操作。
* h624中的分隔符不是 h624规范。属于网络抽象层NAL
* h624视频编码层VCL和网络提取层(网络抽象层)(NAL)
* WebRtc 可以实现点对点进行通信。
* 音视频编辑都是先解除封装格式，然后拿到对应的原始数据进行编辑。
* 音频和视频文件都有自己的分隔符，视频是帧分隔符，音频是时间分隔符。
* MediaExtractor 可以用来分离容器中的视频track和音频track 
    * track 可以获取到 MediaFormat 用于存储track 相关信息。
* MediaCodec 视频编码解码工具。
* 音频可以合成，可以分解为多个波。
* Android mediaMetadataRetriever.getFrameAtTime可以获取到到视频的帧。速度较慢，建议使用缩略图和非目标帧。
* Android AudioSource 用于处理录制音频。mediaFormat 相同的情况下，相同时间段的音频大小相同。
* 当项目中包含 NDK 的时候，默认导入包 会报错。
````aidl
// 和buildTypes 同一级。
ndkVersion '22.1.7171670'
````
* x264 用于解决 media 硬解 性能不统一的问题。

  